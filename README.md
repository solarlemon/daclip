## 	Controlling Vision-Language Models for Universal Image Restoration <br><sub>Official PyTorch Implementation of DA-CLIP. </sub>

[Project Page](https://algolzw.github.io/daclip-uir) | [Paper](https://arxiv.org/abs/2310.01018) | [Model Card ğŸ¤—](https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/daclip-uir-colab/blob/main/daclip_uir_gradio_colab.ipynb) [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/fffiloni/DA-CLIP) [![Replicate](https://replicate.com/cjwbw/daclip-uir/badge)](https://replicate.com/cjwbw/daclip-uir) 

![daclip](figs/teaser.jpg)

### Overview framework:

![daclip](figs/overview.jpg)

### Updates

[**2023.10.25**] Added [dataset links](https://github.com/Algolzw/daclip-uir#dataset-links) for training and testing. <br>
[**2023.10.13**] Added the Replicate [demo](https://replicate.com/cjwbw/daclip-uir) and [api](https://replicate.com/cjwbw/daclip-uir/api)ğŸ”¥. Thanks to [@chenxwh](https://github.com/chenxwh)!!! We updated the Hugging Face [demo](https://huggingface.co/spaces/fffiloni/DA-CLIP)ğŸ”¥ and online Colab [demo](https://colab.research.google.com/github/camenduru/daclip-uir-colab/blob/main/daclip_uir_gradio_colab.ipynb)ğŸ”¥. Thanks to [@fffiloni](https://github.com/fffiloni) and [@camenduru](https://github.com/camenduru) !!! We also made a [Model Card](https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde) in Hugging Face ğŸ¤— and provided more [examples](https://drive.google.com/file/d/1C1nmP5kJXzxrULxTMVWF5P30qezqP6kn/view?usp=sharing) for testing.<br>
[**2023.10.09**] The **pretrained weights** of DA-CLIP and the Universal IR model are released in [link1](https://drive.google.com/file/d/1A6u4CaVrcpcZckGUNzEXqMF8x_JXsZdX/view?usp=sharing) and [link2](https://drive.google.com/file/d/1eXsyrmAbWOvhIY4Wbt5v4IxaggA5aZMG/view?usp=sharing), respectively. In addition, we also provide a [Gradio](https://gradio.app/) app file for the case that you want to [test your own images](https://github.com/Algolzw/daclip-uir#Gradio).<br>

#### Notice!!

ğŸ™ In testing we found that the current pretrained model is still difficult to process some real-world images  which might have distribution shifts with our training dataset (captured from different devices or with different resolutions or degradations). We regard it as a future work and will try to make our model more practical! We also encourage users who are interested in our work to train their own models with larger dataset and more degradation types.

ğŸ™ BTW, **we also found that directly resizing input images will lead a poor performance for most tasks**. We could try to add the resize step into the training but it always destroys the image quality due to interpolation.

ğŸ™ For the inpainting task our current model only supports face inpainting due to the [dataset limitation](https://github.com/Algolzw/daclip-uir/issues/8#issuecomment-1759528246). We provide our mask [examples](https://github.com/Algolzw/daclip-uir/tree/main/scripts/inpainting_masks) and you can use the [generate\_masked\_face](https://github.com/Algolzw/daclip-uir/blob/main/scripts/generate_masked_face.py) script to generate uncompleted faces.

## ä¸€ã€è¿è¡Œä»£ç 

### ä¾èµ–

* OS: Ubuntu 20.04
* nvidia:
  - cuda: 11.4
* python 3.8

### å®‰è£…ç¯å¢ƒ

We advise you first create a virtual environment with:

```bash
python3 -m venv .env
source .env/bin/activate
pip install -U pip
pip install -r requirements.txt

```

### DA-CLIP Usage

Get into the `universal-image-restoration` directory and run:

```python
import torch
from PIL import Image
import open_clip

checkpoint = 'pretrained/daclip_ViT-B-32.pt'
model, preprocess = open_clip.create_model_from_pretrained('daclip_ViT-B-32', pretrained=checkpoint)
tokenizer = open_clip.get_tokenizer('ViT-B-32')

image = preprocess(Image.open("haze_01.png")).unsqueeze(0)
degradations = ['motion-blurry','hazy','jpeg-compressed','low-light','noisy','raindrop','rainy','shadowed','snowy','uncompleted']
text = tokenizer(degradations)

with torch.no_grad(), torch.cuda.amp.autocast():
    text_features = model.encode_text(text)
    image_features, degra_features = model.encode_image(image, control=True)
    degra_features /= degra_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    text_probs = (100.0 * degra_features @ text_features.T).softmax(dim=-1)
    index = torch.argmax(text_probs[0])

print(f"Task: {task_name}: {degradations[index]} - {text_probs[0][index]}")
```

### äºŒã€æ•°æ®é›†

æŒ‰ç…§æˆ‘ä»¬è®ºæ–‡çš„æ•°æ®é›†æ„é€ éƒ¨åˆ†å‡†å¤‡`è®­ç»ƒ`å’Œ`æµ‹è¯•`æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
#### for training dataset ####
#### (uncompleted means inpainting) ####
datasets/universal/train
|--motion-blurry
|  |--LQ/*.png
|  |--GT/*.png
|--hazy
|--jpeg-compressed
|--low-light
|--noisy
|--raindrop
|--rainy
|--shadowed
|--snowy
|--uncompleted

#### for testing dataset ####
#### (the same structure as train) ####
datasets/universal/val
...

#### for clean captions ####
datasets/universal/daclip_train.csv
datasets/universal/daclip_val.csv
```

ç„¶åè¿›å…¥`universal-image-restoration/config/daclip-sde`ç›®å½•ï¼Œå¹¶åœ¨é€‰é¡¹ä¸­ä¿®æ”¹æ•°æ®é›†è·¯å¾„ã€‚
		æ–‡ä»¶ä½äº`options/train.yml` å’Œ`options/tes.yml`ä¸­ã€‚
		æ‚¨å¯ä»¥å°†æ›´å¤šçš„ä»»åŠ¡æˆ–æ•°æ®é›†æ·»åŠ åˆ°`train`å’Œ`val`ç›®å½•ï¼Œå¹¶å°†é€€åŒ–è¯æ±‡è¯æ·»åŠ åˆ° `distortion`ã€‚	

#### ä¸‹è½½æ•°æ®é›†

| Degradation |                 motion-blurry :trophy: 8.9G                  |                     hazy:trophy: 959.5M                      |                   jpeg-compressed*   å…±27G                   |                    low-light:trophy: 331M                    |                    noisy* (same to jpeg)                     |
| ----------- | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Datasets    | [Gopro](https://drive.google.com/file/d/1y4wvPdOG3mojpFCHTqLgriexhbjoWVkK/view) | [RESIDE-6k](https://drive.google.com/drive/folders/1XVD0x74vKQ0-cqazACUZnjUOWURXIeqH?usp=drive_link) | [DIV2K](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar)+[Flickr2K](https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar) | [LOL](https://drive.google.com/file/d/157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB/view) | [DIV2K](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar)+[Flickr2K](https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar) |

| Degradation |                   raindrop:trophy: 1015.9M                   |                     rainy:trophy: 60.6M                      |                    shadowed:trophy: 457M                     |                          snowy 7.8G                          |                   uncompleted :trophy:297M                   |
| ----------- | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Datasets    | [RainDrop](https://drive.google.com/open?id=1e7R76s6vwUJxILOcAsthgDLPSnOrQ49K) | [Rain100H](http://www.icst.pku.edu.cn/struct/att/Rain100H.zip) | [SRD](https://drive.google.com/file/d/1W8vBRJYDG9imMgr9I2XaA13tlFIEHOjS/view) | [Snow100K](https://www.google.com/url?q=https%3A%2F%2Fdesnownet.s3.amazonaws.com%2Fdataset_synthetic%2Ftrain%2FSnow100K-training.tar.gz&sa=D&sntz=1&usg=AOvVaw1Zj_7kQaF0c26DaZcoKEOr) | [CelebaHQ-256](https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256) |

æ‚¨åªéœ€æå–ç”¨äºè®­ç»ƒçš„è®­ç»ƒæ•°æ®é›†ï¼Œæ‰€æœ‰éªŒè¯æ•°æ®é›†éƒ½å¯ä»¥åœ¨ [Google drive](https://drive.google.com/file/d/1JKd1tA7rMoEbI9190daJqL7i6V1L8KUd/view?usp=sharing)ä¸­ä¸‹è½½ã€‚å¯¹äºjpegå’Œå™ªå£°æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ­¤è„šæœ¬[script](https://github.com/Algolzw/daclip-uir/blob/main/scripts/generate_LQ.py)ç”ŸæˆLQå›¾åƒ

### ä¸‰ã€è®­ç»ƒ

â€‹		æœ¬æ–‡ä¸­ä½¿ç”¨ViTä½œä¸ºç¼–ç å™¨å’Œæ§åˆ¶å™¨çš„é»˜è®¤ä¸»å¹²ã€‚å¦‚å›¾3(a)ä¸­ï¼Œæ§åˆ¶å™¨çš„è¾“å‡ºåŒ…æ‹¬ï¼šåµŒå…¥å±‚å›¾åƒé€€åŒ–$e^I_d$å’Œéšè—æ§ä»¶$h_c$(HQ content)ã€‚éšè—æ§ä»¶ä¸­åŒ…å«æ¥è‡ªtransformerå—ä¸­çš„æ‰€æœ‰è¾“å‡ºï¼Œè¿™äº›è¾“å‡ºéšåè¢«æ·»åŠ åˆ°ç›¸åº”çš„ç¼–ç å™¨å—ä»¥æ§åˆ¶å®ƒä»¬çš„é¢„æµ‹ã€‚Transformer å—ä¹‹é—´çš„è¿æ¥æ˜¯ç®€å•çš„å¯†é›†ç¥ç»ç½‘ç»œï¼Œæ‰€æœ‰å‚æ•°éƒ½åˆå§‹åŒ–ä¸ºé›¶ï¼Œè¿™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å½±å“å›¾åƒç¼–ç å™¨ã€‚ç”±äºè®­ç»ƒæ•°æ®é›†ä¸VLMsä¸­ä½¿ç”¨çš„ç½‘ç»œè§„æ¨¡æ•°æ®é›†ç›¸æ¯”å¾ˆå°ï¼Œå› æ­¤è¿™ç§æ§åˆ¶ç­–ç•¥å¯ä»¥**å‡è½»è¿‡åº¦æ‹Ÿåˆ**ï¼ŒåŒæ—¶**ä¿ç•™åŸå§‹å›¾åƒç¼–ç å™¨çš„åŠŸèƒ½**ã€‚

#### 1.è®­ç»ƒDA-CLIP

> å›¾åƒé€€åŒ–åˆ†ç±»

 [DA-CLIP.md ](da-clip/README.md)  æŸ¥çœ‹è¯¦æƒ…

#### 2.è®­ç»ƒç»Ÿä¸€å›¾åƒæ¢å¤

è®­ç»ƒçš„ä¸»è¦ä»£ç åœ¨`universal-image-restoration/config/daclip-sde` ä¸­ï¼ŒDA-CLIPçš„æ ¸å¿ƒç½‘ç»œæ˜¯åœ¨`universal-image-restoration/open_clip/daclip_model.py`ä¸­

* å°†é¢„å…ˆè®­ç»ƒçš„ [**DA-CLIP weights**](https://drive.google.com/file/d/1A6u4CaVrcpcZckGUNzEXqMF8x_JXsZdX/view?usp=sharing)æƒé‡æ”¾åˆ°é¢„å…ˆè®­ç»ƒçš„ç›®å½•ä¸­ï¼Œå¹¶æ£€æŸ¥daclipè·¯å¾„ã€‚

* ç„¶åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹bashè„šæœ¬è®­ç»ƒæ¨¡å‹ï¼š

```bash
cd universal-image-restoration/config/daclip-sde

# For single GPU:
python3 train.py -opt=options/train.yml

# For distributed training, need to change the gpu_ids in option file
python3 -m torch.distributed.launch --nproc_per_node=2 --master_poer=4321 train.py -opt=options/train.yml --launcher pytorch
```

æ¨¡å‹å’Œè®­ç»ƒæ—¥å¿—å°†ä¿å­˜åœ¨`log/universal-ir`ä¸­ã€‚
æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ`tail -f log/universal-ir/train_universal-ir_***.log -n 100`æ¥æ‰“å°æ—¥å¿—

#### 3.ä¸‹è½½é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹

| Model Name   | Description                                     | GoogleDrive                                                  | HuggingFace                                                  |
| ------------ | ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| DA-CLIP      | Degradation-aware CLIP model                    | [download](https://drive.google.com/file/d/1A6u4CaVrcpcZckGUNzEXqMF8x_JXsZdX/view?usp=sharing) | [download](https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde/blob/main/daclip_ViT-B-32.pt) |
| Universal-IR | DA-CLIP based universal image restoration model | [download](https://drive.google.com/file/d/1eXsyrmAbWOvhIY4Wbt5v4IxaggA5aZMG/view?usp=sharing) | [download](https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde/blob/main/universal-ir.pth) |

### å››ã€è¯„ä¼°

ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œè¯·ä¿®æ”¹è·¯å¾„å’Œæ¨¡å‹è·¯å¾„å¹¶è¿è¡Œ

```bash
cd universal-image-restoration/config/universal-ir
python test.py -opt=options/test.yml
```

Here we provide an [app.py](https://github.com/Algolzw/daclip-uir/tree/main/universal-image-restoration/config/daclip-sde/app.py) file for testing your own images. Before that, you need to download the pretrained weights ([DA-CLIP](https://drive.google.com/file/d/1A6u4CaVrcpcZckGUNzEXqMF8x_JXsZdX/view?usp=sharing) and [UIR](https://drive.google.com/file/d/1eXsyrmAbWOvhIY4Wbt5v4IxaggA5aZMG/view?usp=sharing)) and modify the model path in `options/test.yml`. Then by simply running `python app.py`, you can open `http://localhost:7860` to test the model. (We also provide several images with different degradations in the `images` dir). We also provide more examples from our test dataset in the [google drive](https://drive.google.com/file/d/1C1nmP5kJXzxrULxTMVWF5P30qezqP6kn/view?usp=sharing).	

è¿™é‡Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ª`universal-image-restoration/config/daclip-sde/app.py`æ–‡ä»¶ç”¨äºæµ‹è¯•æ‚¨è‡ªå·±çš„å›¾åƒã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¸‹è½½é¢„å…ˆè®­ç»ƒå¥½çš„æƒé‡([DA-CLIP](https://drive.google.com/file/d/1A6u4CaVrcpcZckGUNzEXqMF8x_JXsZdX/view?usp=sharing) and [UIR](https://drive.google.com/file/d/1eXsyrmAbWOvhIY4Wbt5v4IxaggA5aZMG/view?usp=sharing)) ï¼Œå¹¶åœ¨`options/test.yml`ä¸­ä¿®æ”¹æ¨¡å‹è·¯å¾„ï¼Œç„¶ååªéœ€è¿è¡Œ`python app.py`å³å¯ã€‚æ‰“å¼€http://localhost:7860æµ‹è¯•è¯¥æ¨¡å‹ã€‚(æˆ‘ä»¬è¿˜æä¾›äº†å‡ ä¸ªä¸åŒé™çº§çš„æ¥è‡ª[google drive](https://drive.google.com/file/d/1C1nmP5kJXzxrULxTMVWF5P30qezqP6kn/view?usp=sharing)ä¸­çš„æµ‹è¯•æ•°æ®é›†çš„æ›´å¤šç¤ºä¾‹ã€‚


### äº”ã€Results

![daclip](figs/UIR_results_radar.jpg)

<details>
<summary><strong>Unified Image Restoration</strong> (click to expand) </summary>
![daclip](figs/results-UIR.jpg)

</details>

<details>
<summary><strong>Degradation-Specific Restoration</strong> (click to expand) </summary>


![daclip](figs/results_single.jpg)

</details>



---

**Acknowledgment:** Our DA-CLIP is based on [IR-SDE](https://github.com/Algolzw/image-restoration-sde) and [open_clip](https://github.com/mlfoundations/open_clip). Thanks for their code!

#### Contact

If you have any question, please contact: ziwei.luo@it.uu.se


### Citations

If our code helps your research or work, please consider citing our paper.
The following are BibTeX references:

```
@article{luo2023controlling,
  title={Controlling Vision-Language Models for Universal Image Restoration},
  author={Luo, Ziwei and Gustafsson, Fredrik K and Zhao, Zheng and Sj{\"o}lund, Jens and Sch{\"o}n, Thomas B},
  journal={arXiv preprint arXiv:2310.01018},
  year={2023}
}
```

---


#### --- Thanks for your interest! --- ####

<details>
<summary>statistics</summary>


![visitors](https://visitor-badge.laobi.icu/badge?page_id=Algolzw/daclip-uir)

</details>

